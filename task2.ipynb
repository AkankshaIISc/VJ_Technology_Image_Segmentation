{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c1ebc6",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f81371",
   "metadata": {},
   "source": [
    "### install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50aecb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time  # For tracking training time\n",
    "import numpy as  np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd76d4",
   "metadata": {},
   "source": [
    "### create a custom dataset class to load images and masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56916ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transform=None, resize_size=(224, 224)):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_file = annotation_file\n",
    "        self.transform = transform\n",
    "        self.resize_size = resize_size  # Store resize size\n",
    "        self.images = self._load_images()\n",
    "        self.annotations = self._load_annotations()\n",
    "\n",
    "    def _load_images(self):\n",
    "        with open(self.annotation_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return {img['id']: img['file_name'] for img in data['images']}\n",
    "\n",
    "    def _load_annotations(self):\n",
    "        with open(self.annotation_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return {ann['image_id']: ann for ann in data['annotations']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = list(self.images.keys())[idx]\n",
    "        img_path = os.path.join(self.image_dir, self.images[image_id])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        mask = self._create_mask(image_id, image.size)\n",
    "\n",
    "        # Resize both image and mask\n",
    "        image = image.resize(self.resize_size, Image.BILINEAR)  # Use PIL's resize\n",
    "        mask = Image.fromarray(mask).resize(self.resize_size, Image.NEAREST) # Resize mask\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = torch.from_numpy(mask).long() # Ensure mask is LongTensor\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def _create_mask(self, image_id, image_size):\n",
    "        width, height = image_size\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        anns = [ann for ann in self.annotations.values() if ann['image_id'] == image_id]\n",
    "        for ann in anns:\n",
    "            if isinstance(ann['segmentation'], list):\n",
    "                for polygon in ann['segmentation']:\n",
    "                    polygon_points = [(polygon[i], polygon[i + 1]) for i in range(0, len(polygon), 2)]\n",
    "                    ImageDraw.Draw(Image.fromarray(mask)).polygon(polygon_points, fill=ann['category_id'])\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c9c97",
   "metadata": {},
   "source": [
    "### UNet model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccc830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=None):\n",
    "        super(UNet, self).__init__()\n",
    "        if out_channels is None:\n",
    "            out_channels = 81  # COCO has 80 classes + background\n",
    "        # ... (UNet layers - encoder and decoder blocks) ...\n",
    "        # Example (replace with actual UNet layers):\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv_out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ... (UNet forward pass) ...\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32976ef1",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b892d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, masks in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Validation (optional, but highly recommended)\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Average Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch time: {elapsed_time:.2f} seconds\")\n",
    "        if elapsed_time > 6 * 3600:  # Check for 6-hour limit\n",
    "            print(\"Training stopped: Time limit exceeded.\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610656aa",
   "metadata": {},
   "source": [
    "### Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc730f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Directory: src/train_subset\n",
      "Annotation File: src/subset_train_annotations.json\n",
      "Output Directory: src/masks_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define base directory relative to the notebook location (which is vjti_solution)\n",
    "# This assumes your 'src' folder is directly inside 'vjti_solution'\n",
    "base_dir = 'src'\n",
    "\n",
    "# Construct paths using os.path.join for better portability\n",
    "image_dir = os.path.join(base_dir, 'train_subset')\n",
    "annotation_file = os.path.join(base_dir, 'subset_train_annotations.json')\n",
    "output_dir = os.path.join(base_dir, 'masks_2')\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\") \n",
    "print(f\"Image Directory: {image_dir}\")\n",
    "print(f\"Annotation File: {annotation_file}\")\n",
    "print(f\"Output Directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daf3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 800/800 [04:00<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Train Loss: 0.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Val Loss: 0.0039\n",
      "Epoch time: 266.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 800/800 [04:15<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Train Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Val Loss: 0.0007\n",
      "Epoch time: 547.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 800/800 [04:36<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Train Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Val Loss: 0.0003\n",
      "Epoch time: 849.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 800/800 [04:49<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Train Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Val Loss: 0.0001\n",
      "Epoch time: 1164.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 800/800 [05:01<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Train Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Val Loss: 0.0001\n",
      "Epoch time: 1493.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and data paths (adjust as needed)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "resize_size = (224, 224)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Resize((224, 224), antialias=True), # Resize for efficiency\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "full_dataset = SegmentationDataset(image_dir, annotation_file, transform=transform, resize_size=resize_size)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=64)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=64)\n",
    "\n",
    "# Model, Criterion, Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)  # Replace with your UNet implementation\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Suitable for multi-class segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Save the model (optional)\n",
    "torch.save(trained_model.state_dict(), 'segmentation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c3020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vjti",
   "language": "python",
   "name": "vjti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
